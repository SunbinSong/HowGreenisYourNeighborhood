{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(10000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from fiona import crs\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.crs import CRS\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from shapely.geometry import Point,Polygon\n",
    "import math\n",
    "from osgeo import gdal\n",
    "# from gdalconst import GA_ReadOnly\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score,accuracy_score,balanced_accuracy_score, precision_score,f1_score,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import datetime\n",
    "random.seed(0)\n",
    "np.random.seed(0) \n",
    "%autosave 10\n",
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "##Supressing warnings\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pyproj import CRS\n",
    "\n",
    "# Suppress FutureWarning from pyproj\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyproj.crs.crs\")\n",
    "\n",
    "# Suppress SettingWithCopyWarning from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_img = r'../Processing/green_space_classification\\dist_road\\dist_%s.tif'%ux\n",
    "bands=['ndti','ndre','ndvi','ndwi','mndwi','glcm','B2','B3','B4','B8'] #define the band names in the img\n",
    "\n",
    "# read in all the OSM features that may have vegetation\n",
    "veges = pd.read_excel(r'osm_vegetation_classes.xlsx')\n",
    "\n",
    "foi = ['ndti','ndre','ndvi','ndwi','mndwi','glcm']\n",
    "ind_selected = [bands.index(x) for x in foi]\n",
    "bands = foi\n",
    "\n",
    "accuracy_out = r'../Processing/accuracy_classification.csv'\n",
    "# fail_out = r'D:\\Work/Processing/green_space_classification\\fail_classification.csv'\n",
    "# print(ind_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pnts(row):\n",
    "    # define a function to align samples with raster cell\n",
    "    geometry = row['geometry']\n",
    "    bounds = geometry.bounds\n",
    "    xmin, ymin, xmax, ymax = bounds[0], bounds[1], bounds[2], bounds[3]\n",
    "    x,y= np.mgrid[xmin:xmax+10:10,ymin:ymax+10:10]\n",
    "    x,y = np.vstack([x.ravel(), y.ravel()])\n",
    "    p = pd.DataFrame(list(zip(x,y)))\n",
    "    p[0]=np.floor((p[0]-row['xmin'])/row['xres'])*row['xres']+row['xmin']+row['xres']/2\n",
    "    p[1]=np.floor((p[1]-row['ymin'])/row['yres'])*row['yres']+row['ymin']+row['yres']/2\n",
    "    p['pnt'] = list(set(zip(p[0],p[1])))\n",
    "    p['pnt']  = p['pnt'].apply(Point)\n",
    "    p = gpd.GeoDataFrame(p['pnt'],geometry='pnt',crs=crs.from_epsg(27700))\n",
    "    p = p[p.within(geometry)]\n",
    "    return p['pnt'].apply(lambda x:[x.x,x.y]).values\n",
    "def sample_raster(row,img_array):\n",
    "    # define a function to sample the rasters\n",
    "    y = int(row['y_n'])\n",
    "    x = int(row['x_n'])\n",
    "    if 0 <= y < img_array.shape[1] and 0 <= x < img_array.shape[2]:\n",
    "        res = img_array[:, y, x]\n",
    "    else:\n",
    "        res = np.nan\n",
    "    if np.isnan(res).any():\n",
    "        res = np.nan\n",
    "    return res\n",
    "\n",
    "# def sample_raster(row,img_array):\n",
    "#     # define a function to sample the rasters\n",
    "#     y = int(row['y_n'])\n",
    "#     x = int(row['x_n'])\n",
    "#     res = img_array[:,y,x]\n",
    "#     if np.isnan(res).any():\n",
    "#         res = np.nan\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the selected feature polygons\n",
    "def extract_OSM_polygons(OSM):\n",
    "    shapefile = gpd.read_file(OSM)\n",
    "    shapefile= shapefile.to_crs({'init': 'epsg:27700'})\n",
    "    shapefile['geometry'] = shapefile.geometry.buffer(-10)\n",
    "    shapefile = shapefile[~shapefile.is_empty]\n",
    "    building = shapefile[~shapefile['building'].isnull()]\n",
    "    building.loc[:,'area_length']=(building.area/building.length).values\n",
    "    building.loc[:,'general'] = 'bldg'\n",
    "    shapefile = shapefile[(shapefile['boundary'].isnull())&shapefile['building'].isnull()]\n",
    "    shapefile['FID'] = list(range(0,len(shapefile.index)))\n",
    "    one_city = pd.DataFrame()\n",
    "    for i in veges.index:\n",
    "        sub = pd.DataFrame()\n",
    "        key = veges.loc[i,'Key']\n",
    "        value = veges.loc[i,'Value']\n",
    "        sub['geometry'] = shapefile.loc[shapefile[key]==value,'geometry']\n",
    "        sub['FID'] = shapefile.loc[shapefile[key]==value,'FID']\n",
    "        sub['key'] = key\n",
    "        sub['value']=value\n",
    "        sub['SALID1'] = OSM.split('\\\\')[-1].split('.')[0]\n",
    "        if len(sub.index)>0:\n",
    "            one_city=pd.concat([one_city,sub])\n",
    "    one_city['general']='vegetation'\n",
    "\n",
    "    if len(one_city.index)>0:\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city,geometry='geometry', crs=crs.from_epsg(27700))\n",
    "        one_city_gdf.loc[:,'shape_index']=(one_city_gdf.length/(4*np.sqrt(one_city_gdf.area))).values\n",
    "        one_city_gdf = one_city_gdf.loc[(one_city_gdf.area<=one_city_gdf.area.quantile(0.975))&\n",
    "              (one_city_gdf.area>=one_city_gdf.area.quantile(0.025))&\n",
    "              (one_city_gdf['shape_index']<one_city_gdf['shape_index'].quantile(0.9))]\n",
    "\n",
    "        background=shapefile.loc[~shapefile['FID'].isin(set(one_city['FID'])),['geometry','FID']]\n",
    "        background['general']='other'\n",
    "        background.loc[:,'shape_index']=(background.length/(4*np.sqrt(background.area))).values\n",
    "        background = background.loc[(background.area<=background.area.quantile(0.975))&\n",
    "          (background.area>=background.area.quantile(0.025))&\n",
    "          (background['shape_index']<background['shape_index'].quantile(0.9))]\n",
    "        one_city_gdf = pd.concat([one_city_gdf,building])\n",
    "        one_city_gdf = pd.concat([one_city_gdf,background])\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city_gdf[['general','geometry','shape_index']],geometry='geometry', crs=crs.from_epsg(27700))\n",
    "    one_city_gdf.to_file(driver = 'ESRI Shapefile', filename= r\"../Processing/polygon_%s_%s.shp\"%(city,year))\n",
    "    return one_city_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(one_city_gdf):\n",
    "    global img,city,bands\n",
    "    # sample points to raster grid\n",
    "    raster = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    geoTransform = raster.GetGeoTransform()\n",
    "    one_city_gdf['xmin'] = geoTransform[0]\n",
    "    one_city_gdf['ymin'] = geoTransform[3]\n",
    "    one_city_gdf['xres'] = geoTransform[1]\n",
    "    one_city_gdf['yres'] = geoTransform[5]\n",
    "    one_city_gdf['pnts']=  one_city_gdf.apply(sample_pnts,axis=1)\n",
    "    # attach sample class\n",
    "    all_sample = gpd.GeoDataFrame()\n",
    "    for i in set(one_city_gdf['general']):\n",
    "        xys = one_city_gdf.loc[one_city_gdf['general']==i,'pnts'].values\n",
    "        xys_flat = [item for sublist in xys for item in sublist]\n",
    "        sample_df = pd.DataFrame(xys_flat)\n",
    "        sample_df['coordinates'] = list(zip(sample_df[0],sample_df[1]))\n",
    "        sample_gdf = gpd.GeoDataFrame(sample_df['coordinates'],\n",
    "                                      geometry=gpd.points_from_xy(sample_df[0],sample_df[1]),crs=\"epsg:27700\")\n",
    "        sample_gdf['class'] = i\n",
    "        all_sample = pd.concat([all_sample,sample_gdf])\n",
    " \n",
    "    # attach sample to img grid x,y\n",
    "    all_sample['x']=all_sample.geometry.x\n",
    "    all_sample['y']=all_sample.geometry.y\n",
    "    all_sample['x_n'] = (all_sample['x'] - geoTransform[0])/geoTransform[1]-0.5\n",
    "    all_sample['y_n'] = (all_sample['y'] - geoTransform[3])/geoTransform[5]-0.5\n",
    "    all_sample = all_sample.reset_index()\n",
    "\n",
    "    # remove overlapped samples\n",
    "    land_sample = all_sample[all_sample['class']!='bldg'].copy()\n",
    "    land_sample.drop_duplicates(subset=['x','y'], keep=False, inplace=True)\n",
    "    clean_sample = pd.concat([land_sample,all_sample[all_sample['class']=='bldg'].copy()])\n",
    "    clean_sample['bldg_drop']=0\n",
    "    clean_sample.loc[clean_sample['class']=='bldg','bldg_drop']=1\n",
    "    clean_sample = clean_sample.sort_values('bldg_drop', ascending=True)\n",
    "    clean_sample.drop_duplicates(subset=['x','y'], keep='last', inplace=True)\n",
    "    \n",
    "    # mask out pixels with nan in any band\n",
    "    img_array= np.array(raster.ReadAsArray())\n",
    "    # print(img_array.shape)\n",
    "    img_array = img_array[ind_selected,:,:]\n",
    "    x_d = img_array.shape[1]\n",
    "    y_d = img_array.shape[2]\n",
    "    n_d =  img_array.shape[0]\n",
    "    img_array = img_array.reshape(n_d,x_d*y_d)\n",
    "    img_array[:,np.isnan(img_array).any(axis=0)] = np.nan\n",
    "    img_array = img_array.reshape(n_d,x_d,y_d)\n",
    "    # use the samples to sample the img\n",
    "    \n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x:sample_raster(x,img_array),axis=1)\n",
    "    #remove bad sample based on ndvi\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample['mean_ndvi'] = clean_sample['sample_value'].apply(lambda x:x[bands.index('ndvi')])\n",
    "\n",
    "    # drop any vegetation sample with NDVI less than 0.1\n",
    "    clean_sample.loc[(clean_sample['class']=='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']<=0.02),'sample_value']=np.nan \n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    \n",
    "    # drop any non-vegetation sample with NDVI greater than median NDVI of vegetated samples\n",
    "    v_median = clean_sample.loc[(clean_sample['class']=='vegetation'),'mean_ndvi'].median()\n",
    "    clean_sample.loc[(clean_sample['class']!='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']>=v_median),'sample_value']=np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample = clean_sample.drop(columns=['mean_ndvi','sample_value'],axis=1)\n",
    "    \n",
    "    # PCA transformation of the img\n",
    "    # min-max normalization first\n",
    "    for i in range(0,img_array.shape[0]):\n",
    "        v = img_array[i,:,:]\n",
    "        img_array[i,:,:]=(v-np.nanmin(v))/(np.nanmax(v)-np.nanmin(v))\n",
    "    img_array_pca = np.copy(img_array)\n",
    "    img_array_pca = img_array_pca.reshape((img_array_pca.shape[0],\n",
    "                                           img_array_pca.shape[1]*img_array_pca.shape[2])).transpose()\n",
    "    img_array_pca_valid = img_array_pca[~np.isnan(img_array_pca).any(axis=1)]\n",
    "    pca = PCA(n_components=img_array_pca_valid.shape[1])\n",
    "    pca_res = pca.fit(img_array_pca_valid)\n",
    "    var=np.cumsum(np.round(pca_res.explained_variance_ratio_, decimals=3)*100)\n",
    "    n_pc = sum(var<=90)+1\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pca_reduce = pca.fit_transform(img_array_pca_valid)\n",
    "    pca_reduce = np.multiply(pca_reduce,pca_res.explained_variance_ratio_[:n_pc])\n",
    "\n",
    "    img_reduce = np.copy(img_array[:n_pc,:,:])\n",
    "    img_reduce_re = img_reduce.reshape((img_reduce.shape[0],img_reduce.shape[1]*img_reduce.shape[2])).transpose()\n",
    "    img_reduce_re[~np.isnan(img_reduce_re).any(axis=1)] = pca_reduce\n",
    "    img_reduce_re = img_reduce_re.transpose()\n",
    "    img_reduce = img_reduce_re.reshape((img_reduce.shape[0],img_reduce.shape[1],img_reduce.shape[2]))\n",
    "\n",
    "    img_array = np.copy(img_reduce)\n",
    "    del img_array_pca,img_array_pca_valid,img_reduce_re,img_reduce,pca_reduce\n",
    "    # determine outliers in the samples\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x,img_array),axis=1)\n",
    "    PCAs = list(range(0,n_pc))\n",
    "    for PC in PCAs:\n",
    "        i  = PC\n",
    "        clean_sample[PC] = clean_sample['sample_value'].apply(lambda x:x[i])\n",
    "    clean_sample= clean_sample.drop('sample_value',axis=1)\n",
    "    \n",
    "    for i in set(clean_sample['class']):\n",
    "        X = clean_sample.loc[clean_sample['class']==i,PCAs].values\n",
    "        X = np.array(X.tolist())\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier']=y_pred\n",
    "        outlier_score = clf.negative_outlier_factor_\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier_score'] = (outlier_score-outlier_score.min()) / (outlier_score.max() - outlier_score.min())\n",
    "    clean_sample[PCAs] = clean_sample[PCAs].astype(np.float32)\n",
    "    clean_sample = clean_sample.dropna()\n",
    "    clean_sample = clean_sample.loc[clean_sample['outlier']!=-1]\n",
    "\n",
    "    # random selection of samples\n",
    "    n_vege = len(clean_sample.loc[clean_sample['class']=='vegetation'])\n",
    "    n_other = len(clean_sample.loc[clean_sample['class']=='other'])\n",
    "    n_bldg = len(clean_sample.loc[clean_sample['class']=='bldg'])\n",
    "    n_sample = int(0.2*min(n_vege,n_other+n_bldg))\n",
    "    # print(\"n_sample: \",n_sample)\n",
    "    # print(\"n_vege: \",n_vege)\n",
    "    # print(\"n_other: \",n_other)\n",
    "    # print(\"n_bldg: \",n_bldg)\n",
    "    \n",
    "    if n_sample>=2500:\n",
    "        n_sample = 2500\n",
    "    if n_sample<200:\n",
    "        n_sample = int(1*min(n_vege,n_other+n_bldg))\n",
    "    sub_clean_sample = clean_sample.loc[clean_sample['class']=='vegetation'].sample(n=n_sample,random_state=0)\n",
    "    if n_other>(n_sample/2) and n_bldg>(n_sample/2):\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample/2),random_state=0)])\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample/2),random_state=0)])\n",
    "    else:\n",
    "        if n_other>(n_sample/2) and n_bldg<(n_sample/2):\n",
    "\n",
    "            sub_clean_sample = sub_clean_sample.append(clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_bldg),random_state=0))\n",
    "\n",
    "            sub_clean_sample = sub_clean_sample.append(clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample-n_bldg),random_state=0))\n",
    "\n",
    "        else:\n",
    "\n",
    "            sub_clean_sample = sub_clean_sample.append([clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample-n_other),random_state=0)])\n",
    "            sub_clean_sample = sub_clean_sample.append([clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_other),random_state=0)])\n",
    "    sub_clean_sample.loc[sub_clean_sample['class']=='bldg','class']='other'\n",
    "    ext=sub_clean_sample[['class','geometry']]\n",
    "    ext = ext.to_crs({'init': 'epsg:27700'})\n",
    "    ext.to_file(r\"../Processing/all_sample_%s_%s.shp\"%(city,year))\n",
    "    return sub_clean_sample, img_array,PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample(sub_clean_sample,PCAs):\n",
    "    global city,year\n",
    "    # training testing split\n",
    "    ft = sub_clean_sample[PCAs]\n",
    "    ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
    "\n",
    "    sub_clean_sample['class'] = sub_clean_sample['class'].apply(lambda x:1 if x=='vegetation' else 0)\n",
    "    targets = sub_clean_sample['class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ft, targets, stratify=targets,random_state=0)\n",
    "    X_test_weight = X_test['outlier_score']\n",
    "    X_test = X_test[PCAs]\n",
    "    X_train_weight = X_train['outlier_score']\n",
    "    X_train = X_train[PCAs]\n",
    "    train_exp = X_train.merge(sub_clean_sample,how='inner')\n",
    "    test_exp = X_test.merge(sub_clean_sample,how='inner')\n",
    "    gpd.GeoDataFrame(train_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"../Processing/train_sample_%s_%s.shp\"%(city,year))\n",
    "    gpd.GeoDataFrame(test_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"../Processing/test_sample_%s_%s.shp\"%(city,year))\n",
    "    return X_train,X_test,y_train,y_test,X_train_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score,clf,param_grid,scorers,X_train,X_test,y_train,y_test,fit_params):\n",
    "    global city\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring= ['f1','f1_weighted'], refit=refit_score,\n",
    "                           cv=skf, return_train_score=False,n_jobs=4,verbose=0)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    return grid_search, {'city':city,'datetime':datetime.datetime.now(),\n",
    "                         'accuracy_balanced':balanced_accuracy_score(y_test,y_pred),\n",
    "                        'accuracy':accuracy_score(y_test,y_pred),\n",
    "                        'precision':precision_score(y_test,y_pred),\n",
    "                         'recall':recall_score(y_test,y_pred),\n",
    "                         'f1_score':f1_score(y_test,y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the selected feature polygons\n",
    "def extract_OSM_polygons(OSM, city, year):\n",
    "    shapefile = gpd.read_file(OSM)\n",
    "    shapefile= shapefile.to_crs({'init': 'epsg:27700'})\n",
    "    shapefile['geometry'] = shapefile.geometry.buffer(-10)\n",
    "    shapefile = shapefile[~shapefile.is_empty]\n",
    "    building = shapefile[~shapefile['building'].isnull()]\n",
    "    building.loc[:,'area_length']=(building.area/building.length).values\n",
    "    building.loc[:,'general'] = 'bldg'\n",
    "    shapefile = shapefile[shapefile['building'].isnull()]\n",
    "    shapefile['FID'] = list(range(0,len(shapefile.index)))\n",
    "    one_city = pd.DataFrame()\n",
    "    sub = pd.DataFrame()\n",
    "    sub['geometry'] = shapefile.loc[shapefile['general']=='vegetation','geometry']\n",
    "    sub['FID'] = shapefile.loc[shapefile['general']=='vegetation','FID']\n",
    "    sub['key'] = 'general'\n",
    "    sub['value']='vegetation'\n",
    "    sub['SALID1'] = OSM.split('\\\\')[-1].split('.')[0]\n",
    "    if len(sub.index)>0:\n",
    "        one_city=pd.concat([one_city,sub])\n",
    "    one_city['general']='vegetation'\n",
    "\n",
    "    if len(one_city.index)>0:\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city,geometry='geometry', crs=crs.from_epsg(27700))\n",
    "        one_city_gdf.loc[:,'shape_index']=(one_city_gdf.length/(4*np.sqrt(one_city_gdf.area))).values\n",
    "        one_city_gdf = one_city_gdf.loc[(one_city_gdf.area<=one_city_gdf.area.quantile(0.975))&\n",
    "              (one_city_gdf.area>=one_city_gdf.area.quantile(0.025))&\n",
    "              (one_city_gdf['shape_index']<one_city_gdf['shape_index'].quantile(0.9))]\n",
    "        \n",
    "        background=shapefile.loc[~shapefile['FID'].isin(set(one_city['FID'])),['geometry','FID']]\n",
    "        background['general']='other'\n",
    "        background.loc[:,'shape_index']=(background.length/(4*np.sqrt(background.area))).values\n",
    "        background = background.loc[(background.area<=background.area.quantile(0.975))&\n",
    "          (background.area>=background.area.quantile(0.025))&\n",
    "          (background['shape_index']<background['shape_index'].quantile(0.9))]\n",
    "        one_city_gdf = pd.concat([one_city_gdf,building])\n",
    "        one_city_gdf = pd.concat([one_city_gdf,background])\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city_gdf[['general','geometry','shape_index']],geometry='geometry', crs=crs.from_epsg(27700))\n",
    "    one_city_gdf.to_file(driver = 'ESRI Shapefile', filename= r\"../Processing/polygon_%s_%s.shp\"%(city,year))\n",
    "    return one_city_gdf\n",
    "\n",
    "def grid_search_wrapper(refit_score,clf,param_grid,scorers,X_train,X_test,y_train,y_test,fit_params,city,year):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring= ['f1','f1_weighted'], refit=refit_score,\n",
    "                           cv=skf, return_train_score=False,n_jobs=4,verbose=0)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    return grid_search, {'city':city,'year':year,'datetime':datetime.datetime.now(),\n",
    "                         'accuracy_balanced':balanced_accuracy_score(y_test,y_pred),\n",
    "                        'accuracy':accuracy_score(y_test,y_pred),\n",
    "                        'precision':precision_score(y_test,y_pred),\n",
    "                         'recall':recall_score(y_test,y_pred),\n",
    "                         'f1_score':f1_score(y_test,y_pred)}\n",
    "\n",
    "def generate_sample(one_city_gdf, img, city, year):\n",
    "    logging.info(\"Starting sample generation...\")\n",
    "    global bands\n",
    "    # sample points to raster grid\n",
    "    raster = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    geoTransform = raster.GetGeoTransform()\n",
    "    one_city_gdf['xmin'] = geoTransform[0]\n",
    "    one_city_gdf['ymin'] = geoTransform[3]\n",
    "    one_city_gdf['xres'] = geoTransform[1]\n",
    "    one_city_gdf['yres'] = geoTransform[5]\n",
    "    one_city_gdf['pnts']=  one_city_gdf.apply(sample_pnts,axis=1)\n",
    "    # attach sample class\n",
    "    all_sample = gpd.GeoDataFrame()\n",
    "    for i in set(one_city_gdf['general']):\n",
    "        logging.debug(f\"Processing class: {i}\")\n",
    "        xys = one_city_gdf.loc[one_city_gdf['general']==i,'pnts'].values\n",
    "        xys_flat = [item for sublist in xys for item in sublist]\n",
    "        sample_df = pd.DataFrame(xys_flat)\n",
    "        sample_df['coordinates'] = list(zip(sample_df[0],sample_df[1]))\n",
    "        sample_gdf = gpd.GeoDataFrame(sample_df['coordinates'],\n",
    "                                      geometry=gpd.points_from_xy(sample_df[0],sample_df[1]),crs=\"epsg:27700\")\n",
    "        sample_gdf['class'] = i\n",
    "        logging.debug(f\"Generated {len(sample_gdf)} points for class {i}.\")\n",
    "        all_sample = pd.concat([all_sample,sample_gdf])\n",
    "\n",
    "\n",
    "    print(\"one_city_gdf values\")\n",
    "    print(one_city_gdf['general'].value_counts())\n",
    "    \n",
    "    print(\"all_sample values\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "    \n",
    "    # attach sample to img grid x,y\n",
    "    all_sample['x']=all_sample.geometry.x\n",
    "    all_sample['y']=all_sample.geometry.y\n",
    "    all_sample['x_n'] = (all_sample['x'] - geoTransform[0])/geoTransform[1]-0.5\n",
    "    all_sample['y_n'] = (all_sample['y'] - geoTransform[3])/geoTransform[5]-0.5\n",
    "    all_sample = all_sample.reset_index()\n",
    "\n",
    "    print(\"all_sample values second\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "    \n",
    "    # remove overlapped samples\n",
    "    land_sample = all_sample[all_sample['class']!='bldg'].copy()\n",
    "    print(\"land_sample\")\n",
    "    print(len(land_sample))\n",
    "    print(len(land_sample.drop_duplicates(subset=['x','y'], keep=False, inplace=False)))\n",
    "    land_sample.drop_duplicates(subset=['x','y'], keep=False, inplace=True)\n",
    "    # land_sample.drop_duplicates(subset=['x','y'], keep='first', inplace=True)\n",
    "    clean_sample = pd.concat([land_sample,all_sample[all_sample['class']=='bldg'].copy()])\n",
    "    clean_sample['bldg_drop']=0\n",
    "    clean_sample.loc[clean_sample['class']=='bldg','bldg_drop']=1\n",
    "    clean_sample = clean_sample.sort_values('bldg_drop', ascending=True)\n",
    "    clean_sample.drop_duplicates(subset=['x','y'], keep='last', inplace=True)\n",
    "   \n",
    "    # mask out pixels with nan in any band\n",
    "    img_array= np.array(raster.ReadAsArray())\n",
    "    # print(img_array.shape)\n",
    "    img_array = img_array[ind_selected,:,:]\n",
    "    x_d = img_array.shape[1]\n",
    "    y_d = img_array.shape[2]\n",
    "    n_d =  img_array.shape[0]\n",
    "    img_array = img_array.reshape(n_d,x_d*y_d)\n",
    "    img_array[:,np.isnan(img_array).any(axis=0)] = np.nan\n",
    "    img_array = img_array.reshape(n_d,x_d,y_d)\n",
    "    # use the samples to sample the img\n",
    "\n",
    "    print(\"clean_sample number before ndvi filtering\")\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x:sample_raster(x,img_array),axis=1)\n",
    "    #remove bad sample based on ndvi\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample['mean_ndvi'] = clean_sample['sample_value'].apply(lambda x:x[bands.index('ndvi')])\n",
    "\n",
    "    # drop any vegetation sample with NDVI less than 0.1\n",
    "    # 0.1 > 0.02 \n",
    "    clean_sample.loc[(clean_sample['class']=='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']<=0.02),'sample_value']=np.nan \n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "\n",
    "    print(\"clean_sample number after ndvi filtering\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    # drop any non-vegetation sample with NDVI greater than median NDVI of vegetated samples\n",
    "    v_median = clean_sample.loc[(clean_sample['class']=='vegetation'),'mean_ndvi'].median()\n",
    "    print(\"v_median\")\n",
    "    print(v_median)\n",
    "    clean_sample.loc[(clean_sample['class']!='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']>=v_median),'sample_value']=np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample = clean_sample.drop(columns=['mean_ndvi','sample_value'],axis=1)\n",
    "\n",
    "    print(\"clean_sample number before pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    # PCA transformation of the img\n",
    "    # min-max normalization first\n",
    "    for i in range(0,img_array.shape[0]):\n",
    "        v = img_array[i,:,:]\n",
    "        img_array[i,:,:]=(v-np.nanmin(v))/(np.nanmax(v)-np.nanmin(v))\n",
    "    img_array_pca = np.copy(img_array)\n",
    "    img_array_pca = img_array_pca.reshape((img_array_pca.shape[0],\n",
    "                                           img_array_pca.shape[1]*img_array_pca.shape[2])).transpose()\n",
    "    img_array_pca_valid = img_array_pca[~np.isnan(img_array_pca).any(axis=1)]\n",
    "    pca = PCA(n_components=img_array_pca_valid.shape[1])\n",
    "    pca_res = pca.fit(img_array_pca_valid)\n",
    "    var=np.cumsum(np.round(pca_res.explained_variance_ratio_, decimals=3)*100)\n",
    "    n_pc = sum(var<=90)+1\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pca_reduce = pca.fit_transform(img_array_pca_valid)\n",
    "    pca_reduce = np.multiply(pca_reduce,pca_res.explained_variance_ratio_[:n_pc])\n",
    "\n",
    "    img_reduce = np.copy(img_array[:n_pc,:,:])\n",
    "    img_reduce_re = img_reduce.reshape((img_reduce.shape[0],img_reduce.shape[1]*img_reduce.shape[2])).transpose()\n",
    "    img_reduce_re[~np.isnan(img_reduce_re).any(axis=1)] = pca_reduce\n",
    "    img_reduce_re = img_reduce_re.transpose()\n",
    "    img_reduce = img_reduce_re.reshape((img_reduce.shape[0],img_reduce.shape[1],img_reduce.shape[2]))\n",
    "\n",
    "    img_array = np.copy(img_reduce)\n",
    "    del img_array_pca,img_array_pca_valid,img_reduce_re,img_reduce,pca_reduce\n",
    "    # determine outliers in the samples\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x,img_array),axis=1)\n",
    "    PCAs = list(range(0,n_pc))\n",
    "    for PC in PCAs:\n",
    "        i  = PC\n",
    "        clean_sample[PC] = clean_sample['sample_value'].apply(lambda x:x[i])\n",
    "    clean_sample= clean_sample.drop('sample_value',axis=1)\n",
    "\n",
    "    print(\"clean_sample number after pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    for i in set(clean_sample['class']):\n",
    "        X = clean_sample.loc[clean_sample['class']==i,PCAs].values\n",
    "        X = np.array(X.tolist())\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier']=y_pred\n",
    "        outlier_score = clf.negative_outlier_factor_\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier_score'] = (outlier_score-outlier_score.min()) / (outlier_score.max() - outlier_score.min())\n",
    "    clean_sample[PCAs] = clean_sample[PCAs].astype(np.float32)\n",
    "    clean_sample = clean_sample.dropna()\n",
    "    clean_sample = clean_sample.loc[clean_sample['outlier']!=-1]\n",
    "    \n",
    "    print(\"clean_sample number after second pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    # random selection of samples\n",
    "    n_vege = len(clean_sample.loc[clean_sample['class']=='vegetation'])\n",
    "    n_other = len(clean_sample.loc[clean_sample['class']=='other'])\n",
    "    n_bldg = len(clean_sample.loc[clean_sample['class']=='bldg'])\n",
    "    n_sample = int(0.2*min(n_vege,n_other+n_bldg))\n",
    "    print(\"n_sample: \",n_sample)\n",
    "    print(\"n_vege: \",n_vege)\n",
    "    print(\"n_other: \",n_other)\n",
    "    print(\"n_bldg: \",n_bldg)\n",
    "    \n",
    "    if n_sample>=2500:\n",
    "        n_sample = 2500\n",
    "    if n_sample<200:\n",
    "        n_sample = int(1*min(n_vege,n_other+n_bldg))\n",
    "    sub_clean_sample = clean_sample.loc[clean_sample['class']=='vegetation'].sample(n=n_sample,random_state=0)\n",
    "    if n_other>(n_sample/2) and n_bldg>(n_sample/2):\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample/2),random_state=0)])\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample/2),random_state=0)])\n",
    "    else:\n",
    "        if n_other > (n_sample / 2) and n_bldg < (n_sample / 2):\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'bldg'].sample(n=int(n_bldg), random_state=0)])\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'other'].sample(n=int(n_sample - n_bldg), random_state=0)])\n",
    "        else:\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'bldg'].sample(n=int(n_sample - n_other), random_state=0)])\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'other'].sample(n=int(n_other), random_state=0)])\n",
    "\n",
    "    # else:\n",
    "    #     if n_other>(n_sample/2) and n_bldg<(n_sample/2):\n",
    "\n",
    "    #         sub_clean_sample = sub_clean_sample.append(clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_bldg),random_state=0))\n",
    "\n",
    "    #         sub_clean_sample = sub_clean_sample.append(clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample-n_bldg),random_state=0))\n",
    "\n",
    "    #     else:\n",
    "\n",
    "    #         sub_clean_sample = sub_clean_sample.append([clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample-n_other),random_state=0)])\n",
    "    #         sub_clean_sample = sub_clean_sample.append([clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_other),random_state=0)])\n",
    "    sub_clean_sample.loc[sub_clean_sample['class']=='bldg','class']='other'\n",
    "    ext=sub_clean_sample[['class','geometry']]\n",
    "    ext = ext.to_crs({'init': 'epsg:27700'})\n",
    "    ext.to_file(r\"../Processing/all_sample_%s_%s.shp\"% (city, year))\n",
    "    # encouraging garbage collection\n",
    "    raster = None\n",
    "    return sub_clean_sample, img_array,PCAs\n",
    "\n",
    "def split_sample(sub_clean_sample,PCAs,city,year):\n",
    "    # training testing split\n",
    "    ft = sub_clean_sample[PCAs]\n",
    "    ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
    "\n",
    "    sub_clean_sample['class'] = sub_clean_sample['class'].apply(lambda x:1 if x=='vegetation' else 0)\n",
    "    targets = sub_clean_sample['class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ft, targets, stratify=targets,random_state=0)\n",
    "    X_test_weight = X_test['outlier_score']\n",
    "    X_test = X_test[PCAs]\n",
    "    X_train_weight = X_train['outlier_score']\n",
    "    X_train = X_train[PCAs]\n",
    "    train_exp = X_train.merge(sub_clean_sample,how='inner')\n",
    "    test_exp = X_test.merge(sub_clean_sample,how='inner')\n",
    "    gpd.GeoDataFrame(train_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"../Processing/train_sample_%s_%s.shp\"%(city,year))\n",
    "    gpd.GeoDataFrame(test_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"../Processing/test_sample_%s_%s.shp\"%(city,year))\n",
    "    return X_train,X_test,y_train,y_test,X_train_weight\n",
    "\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import warnings\n",
    "from filelock import FileLock\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def process_city_year(city, year, tif_names):\n",
    "    try:\n",
    "        logging.info(f'Starting processing for {city} in {year}')\n",
    "        OSM = r'../OSM/%s_%s_clipped.shp' % (city, year)\n",
    "        # OSM = r'../OSM/%s.shp' % city\n",
    "        # img = r'../Sample_image/LA_%s_%s.tif' % (city, year)\n",
    "        img = r'../Sample_image/LA_%s_%s.tif' % (tif_names[city], year)\n",
    "        \n",
    "        if not os.path.exists(OSM):\n",
    "            logging.error(f'OSM file does not exist: {OSM}')\n",
    "            return\n",
    "        if not os.path.exists(img):\n",
    "            logging.error(f'Image file does not exist: {img}')\n",
    "            return\n",
    "        \n",
    "        one_city_gdf = extract_OSM_polygons(OSM, city, year) # get OSM polygons\n",
    "        print(\"one_city_gdf after extract\")\n",
    "        print(len(one_city_gdf))\n",
    "        sub_clean_sample, img_array, PCAs = generate_sample(one_city_gdf, img, city, year) # generate random samples\n",
    "        print(\"sub_clean_sample\")\n",
    "        print(len(sub_clean_sample))\n",
    "        print(sub_clean_sample.head())\n",
    "    \n",
    "        X_train, X_test, y_train, y_test, X_train_weight = split_sample(sub_clean_sample, PCAs,city,year) # sample values, train test split\n",
    "        scorers = {\n",
    "            'precision_score': make_scorer(precision_score),\n",
    "            'recall_score': make_scorer(recall_score),\n",
    "            'accuracy_score': make_scorer(accuracy_score),\n",
    "            'f1_score': make_scorer(f1_score)\n",
    "        }\n",
    "        fit_params = {'sample_weight': X_train_weight}\n",
    "        clf = SVC()\n",
    "        param_grid = {'C': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "                      'gamma': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "                      'random_state': [0],\n",
    "                      'class_weight': ['balanced']}\n",
    "        grid_search_clf, test_scores = grid_search_wrapper('f1_weighted', clf, param_grid, scorers, X_train, X_test, y_train, y_test, fit_params,city,year)\n",
    "        \n",
    "        # save testing accuracy\n",
    "        logging.info(f'Test scores for {city} in {year}: {test_scores}')\n",
    "    \n",
    "        lock = FileLock(f\"{accuracy_out}.lock\")\n",
    "        with lock:\n",
    "            with open(accuracy_out, 'a') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter=',', lineterminator='\\n')\n",
    "                writer.writerow([f\"{city}_{year}\"] + list(zip(test_scores.keys(), test_scores.values())))\n",
    "        \n",
    "        csv_file.close()\n",
    "        \n",
    "        # save original img\n",
    "        img_array2 = np.copy(img_array)\n",
    "        img_re = img_array2.reshape((img_array2.shape[0], img_array2.shape[1] * img_array2.shape[2])).transpose()\n",
    "        img_pre = np.copy(img_re[~np.isnan(img_re).any(axis=1)])\n",
    "        img_pre = grid_search_clf.predict(img_pre)\n",
    "        img_pre = img_pre.astype(np.int16)\n",
    "        res = img_re[:, 0]\n",
    "        res[~np.isnan(res)] = img_pre\n",
    "        res[np.isnan(res)] = -32768\n",
    "        res = res.reshape(img_array[0, :, :].shape)\n",
    "        res = res.astype(np.int16)\n",
    "\n",
    "        org_img = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "        meta = {\n",
    "            'driver': 'GTiff',\n",
    "            'dtype': 'int16',\n",
    "            'nodata': -32768,\n",
    "            'width': res.shape[1],\n",
    "            'height': res.shape[0],\n",
    "            'count': 1,\n",
    "            # due to package update\n",
    "            # 'crs': CRS.from_dict(init='epsg:27700'),\n",
    "            'crs': CRS(\"EPSG:27700\"),\n",
    "            'transform': Affine(10, 0.0, org_img.GetGeoTransform()[0], 0, -10, org_img.GetGeoTransform()[-3]),\n",
    "            'compress': 'lzw',\n",
    "            'interleave': 'pixel'\n",
    "        }\n",
    "\n",
    "        # Save to a specific file for each year\n",
    "        result_path = r'../Results/{}_{}.tif'.format(city, year)\n",
    "        with rio.open(result_path, 'w', **meta) as dst:\n",
    "            dst.write(res, 1)\n",
    "\n",
    "        logging.info(f'Processed and saved results for {city} in {year}')\n",
    "\n",
    "        #encouraging garbage collection\n",
    "        del img_array\n",
    "        org_img = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error processing {city} in {year}: {e}', exc_info=True)\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    tif_names = {\"Greater_Manchester\": 'Manchester', \"Greater_London\": 'London', \"West_Midlands\": 'Westmidlands'}\n",
    "    # cities = [\"London\"]\n",
    "    cities = [\"Greater_Manchester\", \"West_Midlands\", \"Greater_London\"]\n",
    "    # cities = [\"Greater_London\"]\n",
    "    years = range(2015, 2024)\n",
    "    \n",
    "    max_workers = 5  # Adjust this number based on your system's CPU cores and memory\n",
    "    tasks = [(city, year) for city in cities for year in years]\n",
    "    # tasks = [task for task in tasks if not task in ((\"Greater_Manchester\", 2015),(\"Greater_Manchester\", 2016))]\n",
    "\n",
    "    # Using ProcessPoolExecutor to handle parallel processing\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_city_year = {\n",
    "            executor.submit(process_city_year, city, year, tif_names): (city, year)\n",
    "            for city, year in tasks\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_city_year):\n",
    "            city, year = future_to_city_year[future]\n",
    "            try:\n",
    "                future.result()  # Raises exception if any occurred during processing\n",
    "                logging.info(f'Completed processing for {city} in {year}')\n",
    "                # gc.collect()\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error processing {city} in {year}: {e}', exc_info=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "print('All jobs done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse",
   "language": "python",
   "name": "lse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
